<!-- Generated by SpecStory, Markdown v2.1.0 -->

<!-- cursor Session f19a7121-e9ee-418c-ab53-1a081fc0064b (2026-01-07 14:12Z) -->

# AI model querying and image generation (2026-01-07 14:12Z)

_**User (2026-01-07 14:05Z)**_

write python code to query gemini, openai, and anthropic models.

and write another script specifically for image generation models.

---

_**Agent (model default, mode Agent)**_



---


Reviewing the workspace, then creating scripts to query Gemini, OpenAI, and Anthropic models, plus a separate script for image generation.


---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: /Users/demo/generic-llm-version-test/prompt.txt</summary>

</details>
</tool-use>

---

<tool-use data-tool-type="generic" data-tool-name="list_dir">
<details>
            <summary>Tool use: **list_dir** • Listed directory, 0 results</summary>
        
No results found
</details>
</tool-use>

---



---


Creating two Python scripts: one for text generation (Gemini, OpenAI, Anthropic) and one for image generation.


---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **write** • Edit file: /Users/demo/generic-llm-version-test/query_llms.py</summary>

**Chunk 1**
Lines added: 202, lines removed: 1

```diff
@@ -1,1 +1,202 @@
- 
+ #!/usr/bin/env python3
+ """
+ Script to query text generation models from Gemini, OpenAI, and Anthropic.
+ Requires API keys to be set in environment variables:
+ - GOOGLE_API_KEY (for Gemini)
+ - OPENAI_API_KEY (for OpenAI)
+ - ANTHROPIC_API_KEY (for Anthropic)
+ """
+ 
+ import os
+ import sys
+ from typing import Optional, Dict, Any
+ 
+ 
+ def query_gemini(prompt: str, model: str = "gemini-1.5-pro") -&gt; Optional[str]:
+     """
+     Query Google Gemini models.
+     
+     Args:
+         prompt: The input prompt
+         model: Model name (e.g., 'gemini-1.5-pro', 'gemini-1.5-flash', 'gemini-pro')
+     
+     Returns:
+         Response text or None if error
+     """
+     try:
+         import google.generativeai as genai
+         
+         api_key = os.getenv("GOOGLE_API_KEY")
+         if not api_key:
+             print("Error: GOOGLE_API_KEY environment variable not set")
+             return None
+         
+         genai.configure(api_key=api_key)
+         model_instance = genai.GenerativeModel(model)
+         
+         response = model_instance.generate_content(prompt)
+         return response.text
+     
+     except ImportError:
+         print("Error: google-generativeai package not installed. Install with: pip install google-generativeai")
+         return None
+     except Exception as e:
+         print(f"Error querying Gemini: {e}")
+         return None
+ 
+ 
+ def query_openai(prompt: str, model: str = "gpt-4o") -&gt; Optional[str]:
+     """
+     Query OpenAI models.
+     
+     Args:
+         prompt: The input prompt
+         model: Model name (e.g., 'gpt-4o', 'gpt-4-turbo', 'gpt-3.5-turbo', 'o1-preview', 'o1-mini')
+     
+     Returns:
+         Response text or None if error
+     """
+     try:
+         from openai import OpenAI
+         
+         api_key = os.getenv("OPENAI_API_KEY")
+         if not api_key:
+             print("Error: OPENAI_API_KEY environment variable not set")
+             return None
+         
+         client = OpenAI(api_key=api_key)
+         
+         response = client.chat.completions.create(
+             model=model,
+             messages=[
+                 {"role": "user", "content": prompt}
+             ]
+         )
+         
+         return response.choices[0].message.content
+     
+     except ImportError:
+         print("Error: openai package not installed. Install with: pip install openai")
+         return None
+     except Exception as e:
+         print(f"Error querying OpenAI: {e}")
+         return None
+ 
+ 
+ def query_anthropic(prompt: str, model: str = "claude-3-5-sonnet-20241022") -&gt; Optional[str]:
+     """
+     Query Anthropic Claude models.
+     
+     Args:
+         prompt: The input prompt
+         model: Model name (e.g., 'claude-3-5-sonnet-20241022', 'claude-3-5-haiku-20241022', 
+                'claude-3-opus-20240229', 'claude-3-sonnet-20240229')
+     
+     Returns:
+         Response text or None if error
+     """
+     try:
+         from anthropic import Anthropic
+         
+         api_key = os.getenv("ANTHROPIC_API_KEY")
+         if not api_key:
+             print("Error: ANTHROPIC_API_KEY environment variable not set")
+             return None
+         
+         client = Anthropic(api_key=api_key)
+         
+         message = client.messages.create(
+             model=model,
+             max_tokens=4096,
+             messages=[
+                 {"role": "user", "content": prompt}
+             ]
+         )
+         
+         return message.content[0].text
+     
+     except ImportError:
+         print("Error: anthropic package not installed. Install with: pip install anthropic")
+         return None
+     except Exception as e:
+         print(f"Error querying Anthropic: {e}")
+         return None
+ 
+ 
+ def main():
+     """Main function to demonstrate usage."""
+     if len(sys.argv) &lt; 2:
+         print("Usage: python query_llms.py '&lt;prompt&gt;' [provider] [model]")
+         print("\nProviders: gemini, openai, anthropic")
+         print("\nExample models:")
+         print("  Gemini: gemini-1.5-pro, gemini-1.5-flash, gemini-pro")
+         print("  OpenAI: gpt-4o, gpt-4-turbo, gpt-3.5-turbo, o1-preview, o1-mini")
+         print("  Anthropic: claude-3-5-sonnet-20241022, claude-3-5-haiku-20241022")
+         print("\nExample:")
+         print("  python query_llms.py 'What is AI?' gemini gemini-1.5-pro")
+         sys.exit(1)
+     
+     prompt = sys.argv[1]
+     provider = sys.argv[2].lower() if len(sys.argv) &gt; 2 else None
+     model = sys.argv[3] if len(sys.argv) &gt; 3 else None
+     
+     # Default models if not specified
+     default_models = {
+         "gemini": "gemini-1.5-pro",
+         "openai": "gpt-4o",
+         "anthropic": "claude-3-5-sonnet-20241022"
+     }
+     
+     if provider:
+         if provider == "gemini":
+             model = model or default_models["gemini"]
+             print(f"Querying Gemini ({model})...")
+             response = query_gemini(prompt, model)
+         elif provider == "openai":
+             model = model or default_models["openai"]
+             print(f"Querying OpenAI ({model})...")
+             response = query_openai(prompt, model)
+         elif provider == "anthropic":
+             model = model or default_models["anthropic"]
+             print(f"Querying Anthropic ({model})...")
+             response = query_anthropic(prompt, model)
+         else:
+             print(f"Unknown provider: {provider}")
+             sys.exit(1)
+         
+         if response:
+             print("\nResponse:")
+             print(response)
+         else:
+             sys.exit(1)
+     else:
+         # Query all providers
+         print("Querying all providers...\n")
+         
+         print("=" * 60)
+         print("GEMINI")
+         print("=" * 60)
+         response = query_gemini(prompt, default_models["gemini"])
+         if response:
+             print(response)
+         print()
+         
+         print("=" * 60)
+         print("OPENAI")
+         print("=" * 60)
+         response = query_openai(prompt, default_models["openai"])
+         if response:
+             print(response)
+         print()
+         
+         print("=" * 60)
+         print("ANTHROPIC")
+         print("=" * 60)
+         response = query_anthropic(prompt, default_models["anthropic"])
+         if response:
+             print(response)
+ 
+ 
+ if __name__ == "__main__":
+     main()
+ 
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **write** • Edit file: /Users/demo/generic-llm-version-test/image_generation.py</summary>

**Chunk 1**
Lines added: 281, lines removed: 1

```diff
@@ -1,1 +1,281 @@
- 
+ #!/usr/bin/env python3
+ """
+ Script to query image generation models from various providers.
+ Supports: OpenAI DALL-E, Stability AI, Midjourney (via Replicate), and others.
+ 
+ Requires API keys to be set in environment variables:
+ - OPENAI_API_KEY (for DALL-E)
+ - STABILITY_API_KEY (for Stability AI)
+ - REPLICATE_API_TOKEN (for Replicate/Midjourney)
+ """
+ 
+ import os
+ import sys
+ from typing import Optional, Dict, Any
+ from pathlib import Path
+ 
+ 
+ def generate_image_openai(prompt: str, model: str = "dall-e-3", size: str = "1024x1024", 
+                           quality: str = "standard", n: int = 1) -&gt; Optional[Dict[str, Any]]:
+     """
+     Generate images using OpenAI DALL-E models.
+     
+     Args:
+         prompt: The image generation prompt
+         model: Model name ('dall-e-3' or 'dall-e-2')
+         size: Image size ('1024x1024', '1792x1024', '1024x1792' for dall-e-3)
+         quality: Image quality ('standard' or 'hd' for dall-e-3)
+         n: Number of images (1 for dall-e-3, 1-10 for dall-e-2)
+     
+     Returns:
+         Dictionary with image URLs and metadata, or None if error
+     """
+     try:
+         from openai import OpenAI
+         
+         api_key = os.getenv("OPENAI_API_KEY")
+         if not api_key:
+             print("Error: OPENAI_API_KEY environment variable not set")
+             return None
+         
+         client = OpenAI(api_key=api_key)
+         
+         if model == "dall-e-3":
+             response = client.images.generate(
+                 model=model,
+                 prompt=prompt,
+                 size=size,
+                 quality=quality,
+                 n=1  # dall-e-3 only supports n=1
+             )
+         else:  # dall-e-2
+             response = client.images.generate(
+                 model=model,
+                 prompt=prompt,
+                 size=size,
+                 n=n
+             )
+         
+         result = {
+             "urls": [img.url for img in response.data],
+             "model": model,
+             "revised_prompt": getattr(response.data[0], 'revised_prompt', None)
+         }
+         
+         return result
+     
+     except ImportError:
+         print("Error: openai package not installed. Install with: pip install openai")
+         return None
+     except Exception as e:
+         print(f"Error generating image with OpenAI: {e}")
+         return None
+ 
+ 
+ def generate_image_stability(prompt: str, model: str = "stable-diffusion-xl-1024-v1-0",
+                              width: int = 1024, height: int = 1024, 
+                              steps: int = 30) -&gt; Optional[Dict[str, Any]]:
+     """
+     Generate images using Stability AI models.
+     
+     Args:
+         prompt: The image generation prompt
+         model: Model name (e.g., 'stable-diffusion-xl-1024-v1-0', 'stable-diffusion-v1-6')
+         width: Image width in pixels
+         height: Image height in pixels
+         steps: Number of inference steps
+     
+     Returns:
+         Dictionary with image data and metadata, or None if error
+     """
+     try:
+         import base64
+         import requests
+         
+         api_key = os.getenv("STABILITY_API_KEY")
+         if not api_key:
+             print("Error: STABILITY_API_KEY environment variable not set")
+             return None
+         
+         api_host = os.getenv('API_HOST', 'https://api.stability.ai')
+         url = f"{api_host}/v1/generation/{model}/text-to-image"
+         
+         headers = {
+             "Accept": "application/json",
+             "Authorization": f"Bearer {api_key}"
+         }
+         
+         data = {
+             "text_prompts[0][text]": prompt,
+             "cfg_scale": 7,
+             "height": height,
+             "width": width,
+             "samples": 1,
+             "steps": steps,
+         }
+         
+         response = requests.post(url, headers=headers, data=data)
+         
+         if response.status_code != 200:
+             print(f"Error: API request failed with status {response.status_code}")
+             print(response.text)
+             return None
+         
+         result = response.json()
+         
+         # Save image to file
+         output_dir = Path("generated_images")
+         output_dir.mkdir(exist_ok=True)
+         
+         image_paths = []
+         for i, image in enumerate(result.get("artifacts", [])):
+             image_data = base64.b64decode(image["base64"])
+             image_path = output_dir / f"stability_{model}_{i}.png"
+             with open(image_path, "wb") as f:
+                 f.write(image_data)
+             image_paths.append(str(image_path))
+         
+         return {
+             "image_paths": image_paths,
+             "model": model,
+             "seed": result.get("artifacts", [{}])[0].get("seed")
+         }
+     
+     except ImportError:
+         print("Error: requests package not installed. Install with: pip install requests")
+         return None
+     except Exception as e:
+         print(f"Error generating image with Stability AI: {e}")
+         return None
+ 
+ 
+ def generate_image_replicate(prompt: str, model: str = "stability-ai/sdxl:39ed52f2a78e934b3ba6e2a89f5b1c712de7dfea535525255b1aa35c5565e08b") -&gt; Optional[Dict[str, Any]]:
+     """
+     Generate images using Replicate (supports various models including Midjourney-style).
+     
+     Args:
+         prompt: The image generation prompt
+         model: Model identifier (e.g., 'stability-ai/sdxl:...', 'black-forest-labs/flux-dev', etc.)
+     
+     Returns:
+         Dictionary with image URLs and metadata, or None if error
+     """
+     try:
+         import replicate
+         
+         api_token = os.getenv("REPLICATE_API_TOKEN")
+         if not api_token:
+             print("Error: REPLICATE_API_TOKEN environment variable not set")
+             return None
+         
+         os.environ["REPLICATE_API_TOKEN"] = api_token
+         
+         output = replicate.run(
+             model,
+             input={"prompt": prompt}
+         )
+         
+         # Replicate returns different formats depending on the model
+         if isinstance(output, list):
+             urls = output
+         elif isinstance(output, str):
+             urls = [output]
+         else:
+             urls = [str(output)]
+         
+         return {
+             "urls": urls,
+             "model": model
+         }
+     
+     except ImportError:
+         print("Error: replicate package not installed. Install with: pip install replicate")
+         return None
+     except Exception as e:
+         print(f"Error generating image with Replicate: {e}")
+         return None
+ 
+ 
+ def download_image(url: str, output_path: str) -&gt; bool:
+     """Download an image from a URL to a local file."""
+     try:
+         import requests
+         
+         response = requests.get(url)
+         response.raise_for_status()
+         
+         with open(output_path, "wb") as f:
+             f.write(response.content)
+         
+         return True
+     except Exception as e:
+         print(f"Error downloading image: {e}")
+         return False
+ 
+ 
+ def main():
+     """Main function to demonstrate usage."""
+     if len(sys.argv) &lt; 2:
+         print("Usage: python image_generation.py '&lt;prompt&gt;' [provider] [options]")
+         print("\nProviders:")
+         print("  openai    - OpenAI DALL-E models")
+         print("  stability - Stability AI models")
+         print("  replicate - Replicate (various models including Midjourney-style)")
+         print("\nExamples:")
+         print("  python image_generation.py 'a futuristic city' openai")
+         print("  python image_generation.py 'a beautiful landscape' stability")
+         print("  python image_generation.py 'an abstract painting' replicate")
+         sys.exit(1)
+     
+     prompt = sys.argv[1]
+     provider = sys.argv[2].lower() if len(sys.argv) &gt; 2 else "openai"
+     
+     print(f"Generating image with {provider}...")
+     print(f"Prompt: {prompt}\n")
+     
+     if provider == "openai":
+         result = generate_image_openai(prompt)
+         if result:
+             print("Generated image URLs:")
+             for i, url in enumerate(result["urls"]):
+                 print(f"  {i+1}. {url}")
+                 # Download the image
+                 output_dir = Path("generated_images")
+                 output_dir.mkdir(exist_ok=True)
+                 output_path = output_dir / f"dalle_{i}.png"
+                 if download_image(url, str(output_path)):
+                     print(f"     Downloaded to: {output_path}")
+             if result.get("revised_prompt"):
+                 print(f"\nRevised prompt: {result['revised_prompt']}")
+     
+     elif provider == "stability":
+         result = generate_image_stability(prompt)
+         if result:
+             print("Generated images saved to:")
+             for path in result["image_paths"]:
+                 print(f"  - {path}")
+             if result.get("seed"):
+                 print(f"\nSeed: {result['seed']}")
+     
+     elif provider == "replicate":
+         result = generate_image_replicate(prompt)
+         if result:
+             print("Generated image URLs:")
+             for i, url in enumerate(result["urls"]):
+                 print(f"  {i+1}. {url}")
+                 # Download the image
+                 output_dir = Path("generated_images")
+                 output_dir.mkdir(exist_ok=True)
+                 output_path = output_dir / f"replicate_{i}.png"
+                 if download_image(url, str(output_path)):
+                     print(f"     Downloaded to: {output_path}")
+     
+     else:
+         print(f"Unknown provider: {provider}")
+         print("Available providers: openai, stability, replicate")
+         sys.exit(1)
+ 
+ 
+ if __name__ == "__main__":
+     main()
+ 
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **write** • Edit file: /Users/demo/generic-llm-version-test/requirements.txt</summary>

**Chunk 1**
Lines added: 9, lines removed: 1

```diff
@@ -1,1 +1,9 @@
- 
+ # Requirements for query_llms.py
+ google-generativeai&gt;=0.3.0
+ openai&gt;=1.0.0
+ anthropic&gt;=0.18.0
+ 
+ # Requirements for image_generation.py
+ requests&gt;=2.31.0
+ replicate&gt;=0.20.0
+ 
```

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **write** • Edit file: /Users/demo/generic-llm-version-test/README.md</summary>

**Chunk 1**
Lines added: 77, lines removed: 1

```diff
@@ -1,1 +1,77 @@
- 
+ # LLM and Image Generation Scripts
+ 
+ This repository contains Python scripts to query various LLM providers and image generation models.
+ 
+ ## Setup
+ 
+ 1. Install dependencies:
+ &#96;&#96;&#96;bash
+ pip install -r requirements.txt
+ &#96;&#96;&#96;
+ 
+ 2. Set up API keys as environment variables:
+ &#96;&#96;&#96;bash
+ # For text generation
+ export GOOGLE_API_KEY="your-google-api-key"
+ export OPENAI_API_KEY="your-openai-api-key"
+ export ANTHROPIC_API_KEY="your-anthropic-api-key"
+ 
+ # For image generation
+ export STABILITY_API_KEY="your-stability-api-key"  # Optional
+ export REPLICATE_API_TOKEN="your-replicate-token"  # Optional
+ &#96;&#96;&#96;
+ 
+ ## Usage
+ 
+ ### Text Generation (&#96;query_llms.py&#96;)
+ 
+ Query text generation models from Gemini, OpenAI, and Anthropic.
+ 
+ **Basic usage:**
+ &#96;&#96;&#96;bash
+ # Query all providers
+ python query_llms.py "What is artificial intelligence?"
+ 
+ # Query specific provider
+ python query_llms.py "What is AI?" gemini
+ 
+ # Query specific provider and model
+ python query_llms.py "What is AI?" openai gpt-4o
+ python query_llms.py "What is AI?" anthropic claude-3-5-sonnet-20241022
+ &#96;&#96;&#96;
+ 
+ **Available models:**
+ 
+ - **Gemini**: &#96;gemini-1.5-pro&#96;, &#96;gemini-1.5-flash&#96;, &#96;gemini-pro&#96;
+ - **OpenAI**: &#96;gpt-4o&#96;, &#96;gpt-4-turbo&#96;, &#96;gpt-3.5-turbo&#96;, &#96;o1-preview&#96;, &#96;o1-mini&#96;
+ - **Anthropic**: &#96;claude-3-5-sonnet-20241022&#96;, &#96;claude-3-5-haiku-20241022&#96;, &#96;claude-3-opus-20240229&#96;
+ 
+ ### Image Generation (&#96;image_generation.py&#96;)
+ 
+ Generate images using various providers.
+ 
+ **Basic usage:**
+ &#96;&#96;&#96;bash
+ # OpenAI DALL-E (default)
+ python image_generation.py "a futuristic cityscape at sunset"
+ 
+ # Stability AI
+ python image_generation.py "a beautiful mountain landscape" stability
+ 
+ # Replicate (various models)
+ python image_generation.py "an abstract digital art piece" replicate
+ &#96;&#96;&#96;
+ 
+ **Providers:**
+ - **OpenAI**: DALL-E 2 and DALL-E 3
+ - **Stability AI**: Stable Diffusion models
+ - **Replicate**: Various models including Midjourney-style generators
+ 
+ Generated images are saved to the &#96;generated_images/&#96; directory.
+ 
+ ## Scripts
+ 
+ - &#96;query_llms.py&#96; - Query text generation models
+ - &#96;image_generation.py&#96; - Generate images from various providers
+ - &#96;requirements.txt&#96; - Python dependencies
+ 
```

</details>

</tool-use>

---



---


Checking for linting errors:


---

<tool-use data-tool-type="generic" data-tool-name="read_lints">
<details>
          <summary>Tool use: **read_lints** • Read lints for 2 files</summary>
      
Lint paths:

- `query_llms.py`
- `image_generation.py`

Lint results


**No lint errors found**
</details>
</tool-use>

---



---


Making the scripts executable:


---

<tool-use data-tool-type="shell" data-tool-name="run_terminal_cmd">
<details><summary>Tool use: **run_terminal_cmd** • Run command: chmod +x /Users/demo/generic-llm-version-test/query_llms.py /Users/demo/generic-llm-version-test/image_generation.py</summary>

```bash
chmod +x /Users/demo/generic-llm-version-test/query_llms.py /Users/demo/generic-llm-version-test/image_generation.py
```
</details>
</tool-use>

---



---


Created two Python scripts:

## 1. `query_llms.py` — Text generation models

Queries Gemini, OpenAI, and Anthropic models. Features:
- **Gemini**: `gemini-1.5-pro`, `gemini-1.5-flash`, `gemini-pro`
- **OpenAI**: `gpt-4o`, `gpt-4-turbo`, `gpt-3.5-turbo`, `o1-preview`, `o1-mini`
- **Anthropic**: `claude-3-5-sonnet-20241022`, `claude-3-5-haiku-20241022`, etc.

Usage:
```bash
python query_llms.py "Your prompt" [provider] [model]
```

## 2. `image_generation.py` — Image generation models

Supports multiple image generation providers:
- **OpenAI DALL-E**: DALL-E 2 and DALL-E 3
- **Stability AI**: Stable Diffusion models
- **Replicate**: Various models (including Midjourney-style)

Usage:
```bash
python image_generation.py "Your prompt" [provider]
```

Both scripts:
- Read API keys from environment variables
- Include error handling
- Support command-line usage
- Are executable

Also added:
- `requirements.txt` — dependencies
- `README.md` — setup and usage

Install dependencies with `pip install -r requirements.txt`, then set your API keys as environment variables and run the scripts.

---

